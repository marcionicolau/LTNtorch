{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This notebook is licensed under the MIT License. See the [LICENSE file](https://github.com/tommasocarraro/LTNtorch/blob/main/LICENSE) in the project root for details.\n",
    "\n",
    "## Regression\n",
    "\n",
    "Another important problem in Machine Learning is regression, where a relationship is estimated between one independent variable $X$ and a continuous dependent variable $Y$.\n",
    "The essence of regression is, therefore, to approximate a function $f(x) = y$ by a function $f^*$, given examples $(x_i, y_i)$ such that $f(x_i) = y_i$.\n",
    "\n",
    "In LTN one can model a regression task by defining $f^*$ as a learnable function whose parameter values $\\theta$ are\n",
    "constrained by data. Additionally, a regression task requires a notion of *equality*. We therefore define the\n",
    "predicate $Eq$ as a smooth version of the symbol $=$ in order to turn the constraint $f(x_i) = y_i$ into a smooth\n",
    "optimization problem.\n",
    "\n",
    "In this example, we explore regression using a problem from a real estate dataset with 414 examples, each described in\n",
    "terms of 6 real-numbered features:\n",
    "- the transaction date (converted to a float);\n",
    "- the age of the house;\n",
    "- the distance to the nearest station;\n",
    "- the number of convenience stores in the vicinity;\n",
    "- the latitude and longitude coordinates.\n",
    "\n",
    "The model has to predict the house price given these features in input.\n",
    "\n",
    "For this specific task, LTN uses the following language and grounding:\n",
    "\n",
    "**Domains:**\n",
    "- $samples$, denoting the houses and their features;\n",
    "- $prices$, denoting the house prices.\n",
    "\n",
    "**Variables:**\n",
    "- $x$ for the samples;\n",
    "- $y$ for the prices;\n",
    "- $D(x) = samples$;\n",
    "- $D(y) = prices$.\n",
    "\n",
    "**Functions:**\n",
    "- $f^*(x)$: the regression function that has to be learned;\n",
    "- $D_{in}(f^*) = samples$;\n",
    "- $D_{out}(f^*) = prices$, where $D_{out}(.)$ is a function which returns the domain of the output of a given logical\n",
    "function.\n",
    "\n",
    "**Predicates:**\n",
    "- $Eq(y_1, y_2)$: a smooth equality predicate which measures how similar $y_1$ and $y_2$ are;\n",
    "- $D_{in}(Eq) = prices, prices$.\n",
    "\n",
    "**Axioms:**\n",
    "\n",
    "- $\\forall Diag(x,y) \\text{ } Eq(f^*(x), y)$: the output of $f^*$ should be equal to the ground truth $y$ for each\n",
    "example $x$ given in input.\n",
    "\n",
    "Notice again the use of $Diag$: when grounding $x$ and $y$ onto sequences of values, this is done by obeying a\n",
    "one-to-one correspondence between the sequences. In other words, we aggregate pairs of corresponding samples and prices,\n",
    "instead of any combination thereof.\n",
    "\n",
    "\n",
    "**Grounding:**\n",
    "- $\\mathcal{G}(samples)=\\mathbb{R}^{6}$, samples are described by 6 features;\n",
    "- $\\mathcal{G}(prices)=\\mathbb{R}$;\n",
    "- $\\mathcal{G}(x) \\in \\mathbb{R}^{m \\times 6}, \\mathcal{G}(y) \\in \\mathbb{R}^{m \\times 1}$. Notice that this specification\n",
    "refers to the same number $m$ of examples for $x$ and $y$ due to the above one-to-one correspondence\n",
    "obtained with the use of $Diag$;\n",
    "- $\\mathcal{G}(\\mathrm{eq}(\\mathbf{u}, \\mathbf{v}))=\\exp \\left(-\\alpha \\sqrt{\\sum_{j}\\left(u_{j}-v_{j}\\right)^{2}}\\right)$,\n",
    "where the hyper-parameter $\\alpha$ is a real number that scales how strict the smooth equality is. In this example, we use\n",
    "$\\alpha = 0.05$. Intuitively, the smooth equality is $\\operatorname{exp}(- \\alpha d(\\mathbf{u}, \\mathbf{v}))$, where\n",
    "$d(\\mathbf{u}, \\mathbf{v})$ is the Euclidean distance between $\\mathbf{u}$ and $\\mathbf{v}$. It produces a 1 if the\n",
    "distance is zero; as the distance increases, the result decreases exponentially towards 0. In our example, $\\mathbf{u}$\n",
    "will be a vector containing the results of $f^*$ for $j$ samples, while $\\mathbf{v}$ will contain the ground truths\n",
    "associated to the $j$ samples. Our objective is to maximize the truth degree of this predicate;\n",
    "- $\\mathcal{G}(f^*(x) \\mid \\theta): \\operatorname{MLP}_{\\theta}(x)$, where $MLP_{\\theta}$\n",
    "is a Multi-Layer Perceptron which ends in one neuron corresponding to a price prediction (linear activation).\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Now, let's import the dataset.\n",
    "\n",
    "The real estate dataset has 414 examples with 6 features each. We subdivide the dataset into 330 training examples and\n",
    "84 test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"datasets/real-estate.csv\")\n",
    "data = data.sample(frac=1)  # shuffle\n",
    "\n",
    "x = torch.tensor(\n",
    "    data[\n",
    "        [\n",
    "            \"X1 transaction date\",\n",
    "            \"X2 house age\",\n",
    "            \"X3 distance to the nearest MRT station\",\n",
    "            \"X4 number of convenience stores\",\n",
    "            \"X5 latitude\",\n",
    "            \"X6 longitude\",\n",
    "        ]\n",
    "    ].to_numpy()\n",
    ").float()\n",
    "\n",
    "y = torch.tensor(data[[\"Y house price of unit area\"]].to_numpy()).float()\n",
    "\n",
    "x_train, y_train = x[:330], y[:330]\n",
    "x_test, y_test = x[330:], y[330:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LTN setting\n",
    "\n",
    "In order to define our knowledge base (axioms), we need to define function $f$, predicate $Eq$,\n",
    "universal quantifier, and the `SatAgg` operator.\n",
    "\n",
    "For the quantifier, we use the stable product configuration (seen in the tutorials).\n",
    "\n",
    "For function $f$, we use a simple $MLP$ with two hidden layers.\n",
    "\n",
    "For predicate $Eq$, we use a lambda function which implements the *grounding* defined above.\n",
    "\n",
    "`SatAgg` is defined using the `pMeanError` aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ltn\n",
    "\n",
    "\n",
    "# we define function f\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"This model returns the prediction of the price of an house given in input. The output is linear since we are applying\n",
    "    the model to a regression problem.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_sizes=(6, 8, 8, 1)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.linear_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i]) for i in range(1, len(layer_sizes))]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Method which defines the forward phase of the neural network for our regression task.\n",
    "\n",
    "        :param x: the features of the example\n",
    "        :return: prediction for example x\n",
    "        \"\"\"\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "        out = self.linear_layers[-1](x)\n",
    "        return out\n",
    "\n",
    "\n",
    "f = ltn.Function(MLP())\n",
    "\n",
    "# Equality Predicate - not trainable\n",
    "alpha = 0.05\n",
    "Eq = ltn.Predicate(func=lambda u, v: torch.exp(-alpha * torch.sqrt(torch.sum(torch.square(u - v), dim=1))))\n",
    "\n",
    "# we define the universal quantifier and the SatAgg operator\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Utils\n",
    "\n",
    "Now, we need to define some utility classes and functions.\n",
    "\n",
    "We define a standard PyTorch data loader, which takes as input the dataset and returns a generator of batches of data.\n",
    "In particular, we need a data loader instance for training data and one for testing data.\n",
    "\n",
    "Then, we define functions to evaluate the model performances. The model is evaluated on the test set using the following metrics:\n",
    "- the satisfaction level of the knowledge base: measure the ability of LTN to satisfy the knowledge;\n",
    "- the RMSE (Root Mean Squared Error): measure the quality of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# this is a standard PyTorch DataLoader to load the dataset for the training and testing of the model\n",
    "class DataLoader(object):\n",
    "    def __init__(self, x, y, batch_size=1, shuffle=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.x.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.x.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            x = self.x[idxlist[start_idx:end_idx]]\n",
    "            y = self.y[idxlist[start_idx:end_idx]]\n",
    "\n",
    "            yield x, y\n",
    "\n",
    "\n",
    "# define metrics for evaluation of the model\n",
    "\n",
    "\n",
    "# it computes the overall satisfaction level on the knowledge base using the given data loader (train or test)\n",
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for x_data, y_data in loader:\n",
    "        x = ltn.Variable(\"x\", x_data)\n",
    "        y = ltn.Variable(\"y\", y_data)\n",
    "        mean_sat += Forall(ltn.diag(x, y), Eq(f(x), y)).value\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat\n",
    "\n",
    "\n",
    "# it computes the overall RMSE between the predictions and the ground truth, using the given data loader (train or test)\n",
    "def compute_rmse(loader):\n",
    "    mean_rmse = 0.0\n",
    "    for x, y in loader:\n",
    "        predictions = f.model(x).detach().numpy()\n",
    "        mean_rmse += mean_squared_error(y, predictions, squared=False)\n",
    "    return mean_rmse / len(loader)\n",
    "\n",
    "\n",
    "# create train and test loader\n",
    "train_loader = DataLoader(x_train, y_train, 64, shuffle=True)\n",
    "test_loader = DataLoader(x_test, y_test, 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Learning\n",
    "\n",
    "Let us define $D$ the data set of all examples. The objective function with $\\mathcal{K}=\\{\\forall Diag(x,y) \\text{ } Eq(f^*(x), y)\\}$\n",
    "is given by $\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$.\n",
    "\n",
    "In practice, the optimizer uses the following loss function:\n",
    "\n",
    "$\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)$\n",
    "\n",
    "where $B$ is a mini batch sampled from $D$.\n",
    "\n",
    "In the following, we learn our LTN in the regression task using the satisfaction of the knowledge base as\n",
    "an objective. In other words, we want to learn the parameters $\\theta$ of function $f^*$ in such a way the only\n",
    "axiom in the knowledge base is maximally satisfied. We train our model for 500 epochs and use the `Adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.9997 | Train Sat 0.000 | Test Sat 0.000 | Train RMSE 189.168 | Test RMSE 178.333 \n",
      " epoch 50 | loss 0.2877 | Train Sat 0.704 | Test Sat 0.688 | Train RMSE 8.566 | Test RMSE 9.309 \n",
      " epoch 100 | loss 0.2883 | Train Sat 0.710 | Test Sat 0.689 | Train RMSE 8.817 | Test RMSE 9.262 \n",
      " epoch 150 | loss 0.2822 | Train Sat 0.718 | Test Sat 0.701 | Train RMSE 9.077 | Test RMSE 8.714 \n",
      " epoch 200 | loss 0.2715 | Train Sat 0.718 | Test Sat 0.709 | Train RMSE 8.327 | Test RMSE 8.264 \n",
      " epoch 250 | loss 0.2811 | Train Sat 0.734 | Test Sat 0.708 | Train RMSE 8.626 | Test RMSE 8.222 \n",
      " epoch 300 | loss 0.2721 | Train Sat 0.728 | Test Sat 0.727 | Train RMSE 7.677 | Test RMSE 7.802 \n",
      " epoch 350 | loss 0.2567 | Train Sat 0.736 | Test Sat 0.730 | Train RMSE 7.728 | Test RMSE 7.726 \n",
      " epoch 400 | loss 0.2558 | Train Sat 0.717 | Test Sat 0.737 | Train RMSE 7.949 | Test RMSE 7.628 \n",
      " epoch 450 | loss 0.2684 | Train Sat 0.724 | Test Sat 0.730 | Train RMSE 7.645 | Test RMSE 7.703 \n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(f.parameters(), lr=0.0005)\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (x_data, y_data) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # ground the variables with current batch of data\n",
    "        x = ltn.Variable(\"x\", x_data)  # samples\n",
    "        y = ltn.Variable(\"y\", y_data)  # ground truths\n",
    "        sat_agg = Forall(ltn.diag(x, y), Eq(f(x), y)).value\n",
    "        loss = 1.0 - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # we print metrics every 50 epochs of training\n",
    "    if epoch % 50 == 0:\n",
    "        print(\n",
    "            \" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train RMSE %.3f | Test RMSE %.3f \"\n",
    "            % (\n",
    "                epoch,\n",
    "                train_loss,\n",
    "                compute_sat_level(train_loader),\n",
    "                compute_sat_level(test_loader),\n",
    "                compute_rmse(train_loader),\n",
    "                compute_rmse(test_loader),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice that variables $x$ and $y$ are grounded batch by batch with new data arriving from the data loader. This is exactly what\n",
    "we mean with $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$, where $B$ is a mini-batch sampled by the data loader.\n",
    "\n",
    "Notice also that `SatAgg` is defined by one single axiom and returns the truth value corresponding to the evaluation\n",
    "of the axiom.\n",
    "\n",
    "Note that after 300 epochs the test RMSE is around 7, while at the beginning of the training it was around 62. This shows\n",
    "the power of LTN in learning the regression task only using the satisfaction of a knowledge base as an objective.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
