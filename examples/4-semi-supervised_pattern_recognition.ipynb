{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook is licensed under the MIT License. See the [LICENSE file](https://github.com/tommasocarraro/LTNtorch/blob/main/LICENSE) in the project root for details.\n",
    "\n",
    "## Semi-supervised pattern recognition\n",
    "\n",
    "Let us now explore two, more elaborate, classification tasks, which showcase the\n",
    "benefit of using logical reasoning alongside machine learning.\n",
    "\n",
    "The tasks consist in predicting the sum of two numbers given in input. We have the images for these numbers but we do not\n",
    "know their classes, namely their labels. The only thing we know is their sum. In particular, the first task is simpler\n",
    "and consists in predicting the sum of two single-digit numbers. The second one is more complicated since it consists in\n",
    "predicting the sum of two multi-digits numbers.\n",
    "\n",
    "To create our dataset, we use the popular MNIST dataset, which contains labelled manuscripted digits.\n",
    "\n",
    "**Single digits addition:**\n",
    "consider the predicate $addition(X, Y, n)$, where $X$ and $Y$ are\n",
    "images of digits, and $n$ is a natural number corresponding to the sum of these digits. This predicate should return an\n",
    "estimate of the validity of the addition. For instance, $addition(img(8), img(3), 11)$ is a\n",
    "valid addition, while $addition(img(3), img(3), 5)$ is not. In this example, $img(x)$ means \"an image of a $x$\", where $x$ is\n",
    "a digit label. It has not to be confused with a logical function.\n",
    "\n",
    "**Multi digits addition:**\n",
    "the experiment is extended to numbers with more than one\n",
    "digit. Consider the predicate $addition([img(X_1), img(X_2)],[img(Y_1), img(Y_2)], n)$, where $[img(X_1), img(X_2)]$ and\n",
    "$[img(Y_1), img(Y_2)]$ are lists of images of digits, representing two multi-digit numbers, and $n$ is a natural\n",
    "number corresponding to the sum of the two multi-digit numbers. For instance,\n",
    "$addition([img(2), img(0)], [img(1), (7)], 37)$ is a valid addition, while $addition([img(5), img(4)], [img(9), img(0)], 50)$\n",
    "is not.\n",
    "\n",
    "A natural Neural-Symbolic approach is to seek to learn a single digit classifier and\n",
    "benefit from knowledge readily available about the properties of the addition in this case.\n",
    "For instance, suppose that a predicate $digit(x,d)$ gives the likelihood of an image $x$\n",
    "being of digit $d$. A definition for $addition(img(3), img(8), 11)$ in LTN is:\n",
    "\n",
    "$\\exists d_1, d_2 : d_1 + d_2 = 11 \\text{ } (digit(img(3), d_1) \\land digit(img(8), d_2))$.\n",
    "\n",
    "The above task is made more complicated by not providing labels for the\n",
    "single digit images during training. Instead, training takes place on pairs of images\n",
    "with labels made available for the result only, that is, the sum of the individual\n",
    "labels. The single digit classifier is not explicitly trained by itself. Its output is a\n",
    "latent information which is used by the logic. However, this does not pose a problem\n",
    "for end-to-end Neural-Symbolic systems such as LTN, for which the\n",
    "gradients can propagate through the logical structures.\n",
    "\n",
    "We start by illustrating an LTN theory that can be used to learn the predicate\n",
    "$digit$. The specification of the theory below is for the single digit addition example,\n",
    "although it can be extended easily to the multiple digits case:\n",
    "\n",
    "**Domains:**\n",
    "- $images$, denoting the MNIST digit images;\n",
    "- $results$, denoting the integers which are the labels for the results of the additions;\n",
    "- $digits$, denoting the digits from 0 to 9.\n",
    "\n",
    "**Variables:**\n",
    "- $x, y$ ranging over the MNIST images;\n",
    "- $n$ for the labels, i.e., the results of the additions;\n",
    "- $d_1, d_2$ ranging over digits;\n",
    "- $D(x) = D(y) = images$;\n",
    "- $D(n) = results$;\n",
    "- $D(d_1) = d(d_2) = digits$.\n",
    "\n",
    "**Predicates:**\n",
    "- $digit(x, d)$ for the single digit classifier, where $d$ is a term denoting a digit\n",
    "constant or a digit variable. The classifier should return the probability of an\n",
    "image $x$ being of digit $d$;\n",
    "- $D_{in}(digit) = images,digits$.\n",
    "\n",
    "**Axioms:**\n",
    "\n",
    "Single digit addition:\n",
    "\n",
    "$\\forall Diag(x, y, n) \\text{ } (\\exists d_1, d_2 : d_1 + d_2 = n \\text{ } (digit(x, d_1) \\land digit(y, d_2)))$\n",
    "\n",
    "Multiple digit addition:\n",
    "\n",
    "$\\forall Diag(x_1, x_2, y_1, y_2, n) \\text{ } (\\exists d_1, d_2, d_3, d_4: 10d_1 + d_2 + 10d_3 + d_4 = n \\text{ } (digit(x_1, d_1) \\land digit(x_2, d_2) \\land digit(y_1, d_3) \\land digit(y_2, d_4)))$\n",
    "\n",
    "Notice the use of $Diag$: when grounding $x,y,n$ with three sequences of values,\n",
    "the *i-th* examples of each variable are matching. That is, $(\\mathcal{G}(x)_i, \\mathcal{G}(y)_i, \\mathcal{G}(n)_i)$ is\n",
    "a tuple from our dataset of valid additions. Using the diagonal quantification,\n",
    "LTN aggregates pairs of images and their corresponding result, rather than any\n",
    "combination of images and results.\n",
    "\n",
    "Notice also the guarded quantification: by quantifying only on the \"intermediate labels\" (not given during training)\n",
    "that could add up to the result label\n",
    "(given during training), we incorporate symbolic information into the system.\n",
    "\n",
    "\n",
    "**Grounding:**\n",
    "- $\\mathcal{G}(images)=[0, 1]^{28 \\times 28 \\times 1}$. The MNIST dataset has images of 28 by 28 pixels.\n",
    "The images are grayscale and have just one channel. The RGB pixel values from\n",
    "0 to 255 of the MNIST dataset are scaled to the range [0, 1];\n",
    "- $\\mathcal{G}(results)=\\mathbb{N}$;\n",
    "- $\\mathcal{G}(digits) = {0, 1, \\dots, 9}$;\n",
    "- $\\mathcal{G}(x) \\in[0,1]^{m \\times 28 \\times 28 \\times 1}, \\mathcal{G}(y) \\in[0,1]^{m \\times 28 \\times 28 \\times 1}, \\mathcal{G}(n) \\in \\mathbb{N}^{m}$.\n",
    "Notice the use of the same number $m$ of examples for each of these variables as they are supposed\n",
    "to match one-to-one due to the use of $Diag$;\n",
    "- $\\mathcal{G}\\left(d_{1}\\right)=\\mathcal{G}\\left(d_{2}\\right)=\\langle 0,1, \\ldots, 9\\rangle$;\n",
    "- $\\mathcal{G}(digit \\mid \\theta): x, d \\rightarrow \\operatorname{onehot}(d)^{\\top} \\cdot \\operatorname{softmax}\\left(\\operatorname{CNN}_{\\theta}(x)\\right)$, where $CNN$\n",
    "is a Convolutional Neural Network with 10 output neurons for each class. Notice that, in contrast\n",
    "with the previous examples, $d$ is an integer label; $onehot(d)$ converts it into a\n",
    "one-hot label.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Now, let's import and create the dataset.\n",
    "\n",
    "The MNIST dataset contains 70000 images, subdivided into 60000 examples for training and 10000 for test. For our task,\n",
    "we need to create an ad-hoc dataset starting from the MNIST dataset.\n",
    "\n",
    "For the single digit case, the first 30000 images of the training set are used as left operands for the addition, while\n",
    "the last 30000 images are used as right operands. The sum of the labels of the left and right operands is used as target.\n",
    "The same process is repeated to create the test set.\n",
    "\n",
    "For the multiple digit case, the training set is divided into four groups of 15000 images. The first two groups form the\n",
    "left operands for the additions, while the last two groups the right operands. The target is created in a similar manner\n",
    "to the previous case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def get_mnist_dataset_for_digits_addition(single_digit=True):\n",
    "    \"\"\"It prepares the dataset for the MNIST single digit or multi digits addition example of the LTN paper.\n",
    "\n",
    "    :param single_digit: whether the dataset has to be generated for the single digit or multi digits example (please,\n",
    "    carefully read the examples in the paper to understand the differences between the two).\n",
    "    :return: a tuple of two elements. The first element is the training set, while the second element is the test set.\n",
    "    Both training set and test set are lists that contain the following information:\n",
    "        1. a list [left_operands, right_operands], where left_operands is a list of MNIST images that are used as the\n",
    "        left operand of the addition, while right_operands is a list of MNIST images that are used as the right operand\n",
    "        of the addition;\n",
    "        2. a list containing the summation of the labels of the images contained in the list at point 1. The label of\n",
    "        the left operand is added to the label of the right operand, and the target label is generated. This represents\n",
    "        the target of the digits addition task.\n",
    "    Note that this is the output of the process for the single digit case. In the multi digits case the list at point\n",
    "    1 will have 4 elements since in the multi digits case four digits are involved in each addition (two digits\n",
    "    represent the first operand and two digits the second operand).\n",
    "    \"\"\"\n",
    "    if single_digit:\n",
    "        n_train_examples = 30000\n",
    "        n_test_examples = 5000\n",
    "        n_operands = 2\n",
    "    else:\n",
    "        n_train_examples = 15000\n",
    "        n_test_examples = 2500\n",
    "        n_operands = 4\n",
    "\n",
    "    mnist_train = torchvision.datasets.MNIST(\n",
    "        \"./datasets/\", train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "    mnist_test = torchvision.datasets.MNIST(\n",
    "        \"./datasets/\", train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    train_imgs, train_labels, test_imgs, test_labels = (\n",
    "        mnist_train.data,\n",
    "        mnist_train.targets,\n",
    "        mnist_test.data,\n",
    "        mnist_test.targets,\n",
    "    )\n",
    "\n",
    "    train_imgs, test_imgs = train_imgs / 255.0, test_imgs / 255.0\n",
    "\n",
    "    train_imgs, test_imgs = torch.unsqueeze(train_imgs, 1), torch.unsqueeze(test_imgs, 1)\n",
    "\n",
    "    imgs_operand_train = [\n",
    "        train_imgs[i * n_train_examples : i * n_train_examples + n_train_examples] for i in range(n_operands)\n",
    "    ]\n",
    "    labels_operand_train = [\n",
    "        train_labels[i * n_train_examples : i * n_train_examples + n_train_examples] for i in range(n_operands)\n",
    "    ]\n",
    "\n",
    "    imgs_operand_test = [\n",
    "        test_imgs[i * n_test_examples : i * n_test_examples + n_test_examples] for i in range(n_operands)\n",
    "    ]\n",
    "    labels_operand_test = [\n",
    "        test_labels[i * n_test_examples : i * n_test_examples + n_test_examples] for i in range(n_operands)\n",
    "    ]\n",
    "\n",
    "    if single_digit:\n",
    "        label_addition_train = labels_operand_train[0] + labels_operand_train[1]\n",
    "        label_addition_test = labels_operand_test[0] + labels_operand_test[1]\n",
    "    else:\n",
    "        label_addition_train = (\n",
    "            10 * labels_operand_train[0]\n",
    "            + labels_operand_train[1]\n",
    "            + 10 * labels_operand_train[2]\n",
    "            + labels_operand_train[3]\n",
    "        )\n",
    "\n",
    "        label_addition_test = (\n",
    "            10 * labels_operand_test[0] + labels_operand_test[1] + 10 * labels_operand_test[2] + labels_operand_test[3]\n",
    "        )\n",
    "\n",
    "    train_set = [torch.stack(imgs_operand_train, dim=1), label_addition_train]\n",
    "    test_set = [torch.stack(imgs_operand_test, dim=1), label_addition_test]\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "# single digit dataset\n",
    "single_d_train_set, single_d_test_set = get_mnist_dataset_for_digits_addition(single_digit=True)\n",
    "# multi digit dataset\n",
    "multi_d_train_set, multi_d_test_set = get_mnist_dataset_for_digits_addition(single_digit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Just for illustration, we show the first example of the training set for the single digit case. This should help in\n",
    "understanding how the dataset is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operands are displayed in the following images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 432x288 with 2 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target label (sum) for these operands is: 8\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "first_example_images = single_d_train_set[0][0]\n",
    "first_example_label = single_d_train_set[1][0]\n",
    "\n",
    "print(\"Operands are displayed in the following images:\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "imgplot = plt.imshow(first_example_images[0].permute(1, 2, 0))\n",
    "ax.set_title(\"First operand\")\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "imgplot = plt.imshow(first_example_images[1].permute(1, 2, 0))\n",
    "ax.set_title(\"Second operand\")\n",
    "plt.show()\n",
    "print(\"The target label (sum) for these operands is: %d\" % first_example_label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LTN setting\n",
    "\n",
    "In order to define our knowledge base (axioms), we need to define predicate $digit$, variables $d_1$, $d_2$, $d_3$, $d_4$,\n",
    "connectives, universal and existential quantifiers, and the `SatAgg` operator.\n",
    "\n",
    "For connectives and quantifiers, we use the stable product configuration (seen in the tutorials).\n",
    "\n",
    "For predicate $digit$, we have two models. The first one implements a $CNN$ which outputs the logits for the ten classes of\n",
    "the MNIST dataset, given an image $x$ in input. The second model takes as input a labelled example $(x,d)$, it computes the logits\n",
    "using the first model and then returns the prediction (*softmax*) for class $d$. In other words, it computes the likelihood\n",
    "of image $d$ being of digit $d$.\n",
    "\n",
    "We need two separated models because we need both logits and probabilities. Logits are used to compute the classification\n",
    "accuracy, while probabilities are interpreted as truth values to compute the satisfaction level of the knowledge base.\n",
    "\n",
    "The variables $d_1$, $d_2$, $d_3$, and $d_4$ represent the 10 digit labels of the MNIST dataset, namely they are the sequence $\\langle 0,1, \\ldots, 9\\rangle$.\n",
    "\n",
    "`SatAgg` is defined using the `pMeanError` aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.init import xavier_uniform_, normal_, kaiming_uniform_\n",
    "import ltn\n",
    "\n",
    "# we define the variables\n",
    "d_1 = ltn.Variable(\"d_1\", torch.tensor(range(10)))\n",
    "d_2 = ltn.Variable(\"d_2\", torch.tensor(range(10)))\n",
    "# these are used only in the multi digit case\n",
    "d_3 = ltn.Variable(\"d_3\", torch.tensor(range(10)))\n",
    "d_4 = ltn.Variable(\"d_4\", torch.tensor(range(10)))\n",
    "\n",
    "\n",
    "# we define predicate digit\n",
    "class MNISTConv(torch.nn.Module):\n",
    "    \"\"\"CNN that returns linear embeddings for MNIST images. It is used in the single digit and multi digits addition\n",
    "    examples of the LTN paper.\n",
    "\n",
    "    Args:\n",
    "        conv_channels_sizes: tuple containing the number of channels of the convolutional layers of the model. The first\n",
    "        element of the tuple must be the number of input channels of the first conv layer, while the last element\n",
    "        of the tuple must be the number of output channels of the last conv layer. Specifically, the number of conv\n",
    "        layers constructed is equal to `len(conv_channels_sizes) - 1`;\n",
    "        kernel_sizes: tuple containing the sizes of the kernels used in the conv layers of the architecture;\n",
    "        linear_layers_sizes: tuple containing the sizes of the linear layers used as the final layers of the\n",
    "        architecture. The first element of the tuple must be the number of features in input to the first linear layer,\n",
    "        while the last element of the tuple must be the number of output features of the last linear layer. Specifically,\n",
    "        the number of layers constructed is equal to `len(linear_layers_sizes) - 1`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conv_channels_sizes=(1, 6, 16), kernel_sizes=(5, 5), linear_layers_sizes=(256, 100)):\n",
    "        super(MNISTConv, self).__init__()\n",
    "        self.conv_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Conv2d(conv_channels_sizes[i - 1], conv_channels_sizes[i], kernel_sizes[i - 1])\n",
    "                for i in range(1, len(conv_channels_sizes))\n",
    "            ]\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU()  # relu is used as activation for the conv layers\n",
    "        self.tanh = torch.nn.Tanh()  # tanh is used as activation for the linear layers\n",
    "        self.maxpool = torch.nn.MaxPool2d((2, 2))\n",
    "        self.linear_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Linear(linear_layers_sizes[i - 1], linear_layers_sizes[i])\n",
    "                for i in range(1, len(linear_layers_sizes))\n",
    "            ]\n",
    "        )\n",
    "        self.batch_norm_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.BatchNorm1d(linear_layers_sizes[i]) for i in range(1, len(linear_layers_sizes))]\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.conv_layers:\n",
    "            x = self.relu(conv(x))\n",
    "            x = self.maxpool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        for i in range(len(self.linear_layers)):\n",
    "            x = self.tanh(self.batch_norm_layers[i](self.linear_layers[i](x)))\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        r\"\"\"Initialize the weights of the network.\n",
    "        Weights of conv layers are initialized with the :py:func:`torch.nn.init.kaiming_uniform_` initializer,\n",
    "        weights of linear layers are initialized with the :py:func:`torch.nn.init.xavier_uniform_` initializer,\n",
    "        while biases are initialized with the :py:func:`torch.nn.init.normal_` initializer.\n",
    "        \"\"\"\n",
    "        for layer in self.conv_layers:\n",
    "            kaiming_uniform_(layer.weight)\n",
    "            normal_(layer.bias)\n",
    "\n",
    "        for layer in self.linear_layers:\n",
    "            xavier_uniform_(layer.weight)\n",
    "            normal_(layer.bias)\n",
    "\n",
    "\n",
    "class SingleDigitClassifier(torch.nn.Module):\n",
    "    \"\"\"Model classifying one MNIST digit image into 10 possible classes. It outputs the logits, so it is not a normalized\n",
    "    output. It has a convolutional part in the initial layers of the architecture and a linear part in the last\n",
    "    layers of the architecture. To build the convolutional part a pre-configured convolutional model is used.\n",
    "\n",
    "    Args:\n",
    "        layers_sizes: tuple containing the sizes of the linear layers used as the final layers of the\n",
    "        architecture. The first element of the tuple must be the number of features in input to the first linear layer,\n",
    "        while the last element of the tuple must be the number of output features of the last linear layer. Specifically,\n",
    "        the number of layers constructed is equal to `len(layers_sizes) - 1`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers_sizes=(100, 84, 10)):\n",
    "        super(SingleDigitClassifier, self).__init__()\n",
    "        self.mnistconv = MNISTConv()  # this is the convolutional part of the architecture\n",
    "        self.tanh = torch.nn.Tanh()  # tanh is used as activation for the linear layers\n",
    "        self.linear_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(layers_sizes[i - 1], layers_sizes[i]) for i in range(1, len(layers_sizes))]\n",
    "        )\n",
    "        self.batch_norm_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.BatchNorm1d(layers_sizes[i]) for i in range(1, len(layers_sizes))]\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mnistconv(x)\n",
    "        for i in range(len(self.linear_layers) - 1):\n",
    "            x = self.tanh(self.batch_norm_layers[i](self.linear_layers[i](x)))\n",
    "        return self.linear_layers[-1](x)  # in the last layer a sigmoid or a softmax has to be applied\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights of the linear layers of the network.\n",
    "        Weights are initialized with the :py:func:`torch.nn.init.xavier_uniform_` initializer,\n",
    "        while biases are initialized with the :py:func:`torch.nn.init.normal_` initializer.\n",
    "        \"\"\"\n",
    "        for layer in self.linear_layers:\n",
    "            xavier_uniform_(layer.weight)\n",
    "            normal_(layer.bias)\n",
    "\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label d. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax function to get the probabilities per classes. Finally, it returns only the probability related\n",
    "    to the given class d.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        logits = self.logits_model(x)\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.gather(probs, 1, d)\n",
    "        return out\n",
    "\n",
    "\n",
    "# single digit\n",
    "cnn_s_d = SingleDigitClassifier()\n",
    "Digit_s_d = ltn.Predicate(LogitsToPredicate(cnn_s_d)).to(ltn.device)\n",
    "# multi digit\n",
    "cnn_m_d = SingleDigitClassifier()\n",
    "Digit_m_d = ltn.Predicate(LogitsToPredicate(cnn_m_d)).to(ltn.device)\n",
    "\n",
    "# we define the connectives, quantifiers, and the SatAgg\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Utils\n",
    "\n",
    "Now, we need to define some utility classes and functions.\n",
    "\n",
    "We define a standard PyTorch data loader, which takes as input the dataset and returns a generator of batches of data.\n",
    "In particular, we need a data loader instance for training data and one for testing data. We create these loaders both for\n",
    "the single digit case and the multiple digit case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# this is a standard PyTorch DataLoader to load the dataset for the training and testing of the model\n",
    "class DataLoader(object):\n",
    "    def __init__(self, fold, batch_size=1, shuffle=True):\n",
    "        self.fold = fold\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.fold[0].shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.fold[0].shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            digits = self.fold[0][idxlist[start_idx:end_idx]]\n",
    "            addition_labels = self.fold[1][idxlist[start_idx:end_idx]]\n",
    "\n",
    "            yield digits, addition_labels\n",
    "\n",
    "\n",
    "# create train and test loader\n",
    "single_train_loader = DataLoader(single_d_train_set, 32, shuffle=True)\n",
    "single_test_loader = DataLoader(single_d_test_set, 32, shuffle=False)\n",
    "multi_d_train_loader = DataLoader(multi_d_train_set, 32, shuffle=True)\n",
    "multi_d_test_loader = DataLoader(multi_d_test_set, 32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Learning\n",
    "\n",
    "Let us define $D$ the data set of all examples. The objective function is given by $\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$.\n",
    "\n",
    "In practice, the optimizer uses the following loss function:\n",
    "\n",
    "$\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)$\n",
    "\n",
    "where $B$ is a mini batch sampled from $D$.\n",
    "\n",
    "### Single digit\n",
    "\n",
    "In the following, we learn our LTN for the single digit case. We train our model for 20 epochs and use the `Adam`\n",
    "optimizer with a batch size of 32.\n",
    "\n",
    "Note that hyper-parameter $p$ for existential quantification follows a schedule, changing from $p=1$ to $p=6$ gradually\n",
    "with the number of training epochs. If you are interested in the motivation behind this choice, consider\n",
    "reading the paper.\n",
    "\n",
    "The accuracy is measured by predicting the digit values using the predicate $digit$ and reporting the ratio of\n",
    "examples for which the addition is correct. Within each epoch, satisfaction level and accuracy on both training and\n",
    "test data are visualized.\n",
    "\n",
    "The following figure shows the LTN computational graph for the single digit case.\n",
    "\n",
    "![Computational graph](./images/semisupervised-single.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kq/9dcsn6lx3v98hfjw2hytdpqm0000gn/T/ipykernel_2025/2845273327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             Exists(\n\u001b[1;32m     26\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0md_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mAnd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDigit_s_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDigit_s_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mcond_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_z\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mcond_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/ltn/core.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# the management of the input is left to the model or the lambda function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproc_objs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# check if output of predicate contains only truth values, namely values in the range [0., 1.]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kq/9dcsn6lx3v98hfjw2hytdpqm0000gn/T/ipykernel_2025/2377477053.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, d)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kq/9dcsn6lx3v98hfjw2hytdpqm0000gn/T/ipykernel_2025/2377477053.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnistconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/kq/9dcsn6lx3v98hfjw2hytdpqm0000gn/T/ipykernel_2025/2377477053.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/venv/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/venv/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(Digit_s_d.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # scheduling of the parameter p for the existential quantifier as described in the LTN paper\n",
    "    if epoch in range(0, 4):\n",
    "        p = 1\n",
    "    if epoch in range(4, 8):\n",
    "        p = 2\n",
    "    if epoch in range(8, 12):\n",
    "        p = 4\n",
    "    if epoch in range(12, 20):\n",
    "        p = 6\n",
    "    train_loss, test_loss = 0.0, 0.0\n",
    "    train_sat, test_sat = 0.0, 0.0\n",
    "    train_acc, test_acc = 0.0, 0.0\n",
    "    # train step\n",
    "    for batch_idx, (operand_images, addition_label) in enumerate(single_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # ground variables with current batch data\n",
    "        images_x = ltn.Variable(\"x\", operand_images[:, 0])\n",
    "        images_y = ltn.Variable(\"y\", operand_images[:, 1])\n",
    "        labels_z = ltn.Variable(\"z\", addition_label)\n",
    "        sat_agg = Forall(\n",
    "            ltn.diag(images_x, images_y, labels_z),\n",
    "            Exists(\n",
    "                [d_1, d_2],\n",
    "                And(Digit_s_d(images_x, d_1), Digit_s_d(images_y, d_2)),\n",
    "                cond_vars=[d_1, d_2, labels_z],\n",
    "                cond_fn=lambda d1, d2, z: torch.eq(d1.value + d2.value, z.value),\n",
    "                p=p,\n",
    "            ),\n",
    "        ).value\n",
    "        loss = 1.0 - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_sat += sat_agg\n",
    "        # compute train accuracy\n",
    "        predictions_x = torch.argmax(cnn_s_d(operand_images[:, 0].to(ltn.device)), dim=1)\n",
    "        predictions_y = torch.argmax(cnn_s_d(operand_images[:, 1].to(ltn.device)), dim=1)\n",
    "        predictions = predictions_x + predictions_y\n",
    "        train_acc += torch.count_nonzero(torch.eq(addition_label.to(ltn.device), predictions)) / predictions.shape[0]\n",
    "    train_loss = train_loss / len(single_train_loader)\n",
    "    train_sat = train_sat / len(single_train_loader)\n",
    "    train_acc = train_acc / len(single_train_loader)\n",
    "\n",
    "    # test step\n",
    "    for batch_idx, (operand_images, addition_label) in enumerate(single_test_loader):\n",
    "        # ground variables with current batch data\n",
    "        images_x = ltn.Variable(\"x\", operand_images[:, 0])\n",
    "        images_y = ltn.Variable(\"y\", operand_images[:, 1])\n",
    "        labels_z = ltn.Variable(\"z\", addition_label)\n",
    "        sat_agg = Forall(\n",
    "            ltn.diag(images_x, images_y, labels_z),\n",
    "            Exists(\n",
    "                [d_1, d_2],\n",
    "                And(Digit_s_d(images_x, d_1), Digit_s_d(images_y, d_2)),\n",
    "                cond_vars=[d_1, d_2, labels_z],\n",
    "                cond_fn=lambda d1, d2, z: torch.eq(d1.value + d2.value, z.value),\n",
    "                p=p,\n",
    "            ),\n",
    "        ).value\n",
    "        loss = 1.0 - sat_agg\n",
    "        test_loss += loss.item()\n",
    "        test_sat += sat_agg\n",
    "        # compute test accuracy\n",
    "        predictions_x = torch.argmax(cnn_s_d(operand_images[:, 0].to(ltn.device)), dim=1)\n",
    "        predictions_y = torch.argmax(cnn_s_d(operand_images[:, 1].to(ltn.device)), dim=1)\n",
    "        predictions = predictions_x + predictions_y\n",
    "        test_acc += torch.count_nonzero(torch.eq(addition_label.to(ltn.device), predictions)) / predictions.shape[0]\n",
    "    test_loss = test_loss / len(single_test_loader)\n",
    "    test_sat = test_sat / len(single_test_loader)\n",
    "    test_acc = test_acc / len(single_test_loader)\n",
    "\n",
    "    # we print metrics every epoch of training\n",
    "\n",
    "    print(\n",
    "        \" epoch %d | Train loss %.4f | Train Sat %.4f | Train Acc %.4f | Test loss %.4f | Test Sat %.4f |\"\n",
    "        \" Test Acc %.4f \" % (epoch, train_loss, train_sat, train_acc, test_loss, test_sat, test_acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice that variables $x$, $y$, and $n$ are grounded batch by batch with new data arriving from the data loader. This is exactly what\n",
    "we mean with $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$, where $B$ is a mini-batch sampled by the data loader.\n",
    "\n",
    "Notice also that `SatAgg`, differently from previous examples, is specified by one single axiom.\n",
    "\n",
    "This example shows the power of integrating neural networks with logical reasoning.\n",
    "\n",
    "### Multi digit\n",
    "\n",
    "The multi digit case is equal to the single digit case. The only difference is on how the variables are grounded to take\n",
    "into account more digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kq/9dcsn6lx3v98hfjw2hytdpqm0000gn/T/ipykernel_2025/2561407575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         sat_agg = Forall(\n\u001b[1;32m     26\u001b[0m             \u001b[0mltn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_y1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_y2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             Exists(\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0md_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 And(\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/ltn/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, vars, formula, cond_vars, cond_fn, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;31m# we perform the desired quantification after the mask has been applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0maggregation_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfree_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maggregation_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_formula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;31m# For some values in the formula, the mask can result in aggregating with empty variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/LTNtorch/ltn/fuzzy_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, xs, dim, keepdim, p, stable)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(Digit_m_d.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    # scheduling of the parameter p for the existential quantifier as described in the LTN paper\n",
    "    if epoch in range(0, 4):\n",
    "        p = 1\n",
    "    if epoch in range(4, 8):\n",
    "        p = 2\n",
    "    if epoch in range(8, 12):\n",
    "        p = 4\n",
    "    if epoch in range(12, 20):\n",
    "        p = 6\n",
    "    train_loss, test_loss = 0.0, 0.0\n",
    "    train_sat, test_sat = 0.0, 0.0\n",
    "    train_acc, test_acc = 0.0, 0.0\n",
    "    # train step\n",
    "    for batch_idx, (operand_images, addition_label) in enumerate(multi_d_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # ground variables with current batch data\n",
    "        images_x1 = ltn.Variable(\"x1\", operand_images[:, 0])\n",
    "        images_x2 = ltn.Variable(\"x2\", operand_images[:, 1])\n",
    "        images_y1 = ltn.Variable(\"y1\", operand_images[:, 2])\n",
    "        images_y2 = ltn.Variable(\"y2\", operand_images[:, 3])\n",
    "        labels_z = ltn.Variable(\"z\", addition_label)\n",
    "        sat_agg = Forall(\n",
    "            ltn.diag(images_x1, images_x2, images_y1, images_y2, labels_z),\n",
    "            Exists(\n",
    "                [d_1, d_2, d_3, d_4],\n",
    "                And(\n",
    "                    And(Digit_m_d(images_x1, d_1), Digit_m_d(images_x2, d_2)),\n",
    "                    And(Digit_m_d(images_y1, d_3), Digit_m_d(images_y2, d_4)),\n",
    "                ),\n",
    "                cond_vars=[d_1, d_2, d_3, d_4, labels_z],\n",
    "                cond_fn=lambda d1, d2, d3, d4, z: torch.eq(\n",
    "                    10 * d1.value + d2.value + 10 * d3.value + d4.value, z.value\n",
    "                ),\n",
    "                p=p,\n",
    "            ),\n",
    "        ).value\n",
    "        loss = 1.0 - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_sat += sat_agg\n",
    "        # compute train accuracy\n",
    "        predictions_x1 = torch.argmax(cnn_m_d(operand_images[:, 0].to(ltn.device)), dim=1)\n",
    "        predictions_x2 = torch.argmax(cnn_m_d(operand_images[:, 1].to(ltn.device)), dim=1)\n",
    "        predictions_y1 = torch.argmax(cnn_m_d(operand_images[:, 2].to(ltn.device)), dim=1)\n",
    "        predictions_y2 = torch.argmax(cnn_m_d(operand_images[:, 3].to(ltn.device)), dim=1)\n",
    "        predictions = 10 * predictions_x1 + predictions_x2 + 10 * predictions_y1 + predictions_y2\n",
    "        train_acc += torch.count_nonzero(torch.eq(addition_label.to(ltn.device), predictions)) / predictions.shape[0]\n",
    "    train_loss = train_loss / len(multi_d_train_loader)\n",
    "    train_sat = train_sat / len(multi_d_train_loader)\n",
    "    train_acc = train_acc / len(multi_d_train_loader)\n",
    "\n",
    "    # test step\n",
    "    for batch_idx, (operand_images, addition_label) in enumerate(multi_d_test_loader):\n",
    "        # ground variables with current batch data\n",
    "        images_x1 = ltn.Variable(\"x1\", operand_images[:, 0])\n",
    "        images_x2 = ltn.Variable(\"x2\", operand_images[:, 1])\n",
    "        images_y1 = ltn.Variable(\"y1\", operand_images[:, 2])\n",
    "        images_y2 = ltn.Variable(\"y2\", operand_images[:, 3])\n",
    "        labels_z = ltn.Variable(\"z\", addition_label)\n",
    "        sat_agg = Forall(\n",
    "            ltn.diag(images_x1, images_x2, images_y1, images_y2, labels_z),\n",
    "            Exists(\n",
    "                [d_1, d_2, d_3, d_4],\n",
    "                And(\n",
    "                    And(Digit_m_d(images_x1, d_1), Digit_m_d(images_x2, d_2)),\n",
    "                    And(Digit_m_d(images_y1, d_3), Digit_m_d(images_y2, d_4)),\n",
    "                ),\n",
    "                cond_vars=[d_1, d_2, d_3, d_4, labels_z],\n",
    "                cond_fn=lambda d1, d2, d3, d4, z: torch.eq(\n",
    "                    10 * d1.value + d2.value + 10 * d3.value + d4.value, z.value\n",
    "                ),\n",
    "                p=p,\n",
    "            ),\n",
    "        ).value\n",
    "        loss = 1.0 - sat_agg\n",
    "        test_loss += loss.item()\n",
    "        test_sat += sat_agg\n",
    "        # compute test accuracy\n",
    "        predictions_x1 = torch.argmax(cnn_m_d(operand_images[:, 0].to(ltn.device)), dim=1)\n",
    "        predictions_x2 = torch.argmax(cnn_m_d(operand_images[:, 1].to(ltn.device)), dim=1)\n",
    "        predictions_y1 = torch.argmax(cnn_m_d(operand_images[:, 2].to(ltn.device)), dim=1)\n",
    "        predictions_y2 = torch.argmax(cnn_m_d(operand_images[:, 3].to(ltn.device)), dim=1)\n",
    "        predictions = 10 * predictions_x1 + predictions_x2 + 10 * predictions_y1 + predictions_y2\n",
    "        test_acc += torch.count_nonzero(torch.eq(addition_label.to(ltn.device), predictions)) / predictions.shape[0]\n",
    "    test_loss = test_loss / len(multi_d_test_loader)\n",
    "    test_sat = test_sat / len(multi_d_test_loader)\n",
    "    test_acc = test_acc / len(multi_d_test_loader)\n",
    "\n",
    "    # we print metrics every epoch of training\n",
    "\n",
    "    print(\n",
    "        \" epoch %d | Train loss %.4f | Train Sat %.4f | Train Acc %.4f | Test loss %.4f | Test Sat %.4f |\"\n",
    "        \" Test Acc %.4f \" % (epoch, train_loss, train_sat, train_acc, test_loss, test_sat, test_acc)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice that the $\\land$ operator is a binary operator. It is for this reason that we had to create a nested logical\n",
    "conjunction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
